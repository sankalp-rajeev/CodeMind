{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß™ Gemma 3 12B Test Generation Fine-tuning\n",
                "\n",
                "**Goal**: Fine-tune Gemma 3 12B for unit test generation with comprehensive evaluation\n",
                "\n",
                "**Features**:\n",
                "- ‚úÖ Multiple datasets: CodeRM-UnitTest + HumanEval + MBPP (~20k samples)\n",
                "- ‚úÖ Train/Validation split (95%/5%)\n",
                "- ‚úÖ Comprehensive baseline evaluation BEFORE fine-tuning\n",
                "- ‚úÖ Post-training evaluation with improvement metrics\n",
                "- ‚úÖ GGUF export for local deployment (RTX 4080 compatible)\n",
                "\n",
                "**Evaluation Metrics**:\n",
                "- Syntax Validity (compiles)\n",
                "- Structural Quality (has test functions, assertions, imports)\n",
                "- Executability (runs without crash)\n",
                "- Pass Rate (test passes on correct code)\n",
                "- CodeBLEU (semantic similarity)\n",
                "\n",
                "**Hardware**: A100/H100 recommended (40GB+ VRAM)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ 1. Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install unsloth\n",
                "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
                "!pip install datasets transformers trl accelerate bitsandbytes\n",
                "!pip install codebleu  # For CodeBLEU metric"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîë 2. Hugging Face Login"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import login\n",
                "import os\n",
                "\n",
                "# Option 1: Use environment variable\n",
                "hf_token = os.environ.get(\"HF_TOKEN\", None)\n",
                "\n",
                "# Option 2: Use Colab secrets (if available)\n",
                "if not hf_token:\n",
                "    try:\n",
                "        from google.colab import userdata\n",
                "        hf_token = userdata.get('HF_TOKEN')\n",
                "    except:\n",
                "        pass\n",
                "\n",
                "# Option 3: Manual input\n",
                "if not hf_token:\n",
                "    hf_token = input(\"Enter your Hugging Face token: \")\n",
                "\n",
                "login(token=hf_token)\n",
                "print(\"‚úÖ Logged in to Hugging Face\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîß 3. Load Base Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastModel\n",
                "\n",
                "model, tokenizer = FastModel.from_pretrained(\n",
                "    model_name=\"unsloth/gemma-3-12b-it\",\n",
                "    max_seq_length=4096,\n",
                "    load_in_4bit=True,\n",
                "    load_in_8bit=False,\n",
                "    full_finetuning=False,\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ Model loaded: Gemma 3 12B\")\n",
                "print(f\"   Max seq length: 4096\")\n",
                "print(f\"   Quantization: 4-bit\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä 4. Load All Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset, concatenate_datasets\n",
                "import json\n",
                "\n",
                "print(\"üì• Loading datasets...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# ============================================\n",
                "# 1. CodeRM-UnitTest - High-quality unit tests (~17k)\n",
                "# ============================================\n",
                "print(\"\\n1Ô∏è‚É£ Loading CodeRM-UnitTest...\")\n",
                "try:\n",
                "    coderm = load_dataset(\"KAKA22/CodeRM-UnitTest\", split=\"train\")\n",
                "    print(f\"   ‚úÖ CodeRM: {len(coderm)} samples\")\n",
                "    print(f\"   Fields: {coderm.column_names}\")\n",
                "except Exception as e:\n",
                "    print(f\"   ‚ùå Failed: {e}\")\n",
                "    coderm = None\n",
                "\n",
                "# ============================================\n",
                "# 2. HumanEval-MBPP TestGen QA (if available)\n",
                "# ============================================\n",
                "print(\"\\n2Ô∏è‚É£ Loading HumanEval-MBPP-TestGen-QA...\")\n",
                "try:\n",
                "    testgen_qa = load_dataset(\"OllieStanley/humaneval-mbpp-testgen-qa\", split=\"train\")\n",
                "    print(f\"   ‚úÖ TestGen-QA: {len(testgen_qa)} samples\")\n",
                "    print(f\"   Fields: {testgen_qa.column_names}\")\n",
                "except Exception as e:\n",
                "    print(f\"   ‚ö†Ô∏è Failed (optional): {e}\")\n",
                "    testgen_qa = None\n",
                "\n",
                "# ============================================\n",
                "# 3. OpenAI HumanEval - 164 hand-crafted problems\n",
                "# ============================================\n",
                "print(\"\\n3Ô∏è‚É£ Loading OpenAI HumanEval...\")\n",
                "try:\n",
                "    humaneval = load_dataset(\"openai_humaneval\", split=\"test\")\n",
                "    print(f\"   ‚úÖ HumanEval: {len(humaneval)} samples\")\n",
                "    print(f\"   Fields: {humaneval.column_names}\")\n",
                "except Exception as e:\n",
                "    print(f\"   ‚ö†Ô∏è Failed (optional): {e}\")\n",
                "    humaneval = None\n",
                "\n",
                "# ============================================\n",
                "# 4. MBPP - ~1000 Python problems\n",
                "# ============================================\n",
                "print(\"\\n4Ô∏è‚É£ Loading MBPP...\")\n",
                "try:\n",
                "    mbpp = load_dataset(\"Muennighoff/mbpp\", split=\"train\")\n",
                "    print(f\"   ‚úÖ MBPP: {len(mbpp)} samples\")\n",
                "    print(f\"   Fields: {mbpp.column_names}\")\n",
                "except Exception as e:\n",
                "    print(f\"   ‚ö†Ô∏è Failed (optional): {e}\")\n",
                "    mbpp = None\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview dataset samples\n",
                "print(\"DATASET STRUCTURE PREVIEW\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "if coderm:\n",
                "    print(\"\\nüìã CodeRM sample keys:\", coderm[0].keys())\n",
                "    \n",
                "if testgen_qa:\n",
                "    print(\"\\nüìã TestGen-QA sample:\")\n",
                "    print(testgen_qa[0])\n",
                "\n",
                "if humaneval:\n",
                "    print(\"\\nüìã HumanEval sample keys:\", humaneval[0].keys())\n",
                "    \n",
                "if mbpp:\n",
                "    print(\"\\nüìã MBPP sample keys:\", mbpp[0].keys())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîÑ 5. Format Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def format_coderm(example):\n",
                "    \"\"\"Format CodeRM-UnitTest dataset\"\"\"\n",
                "    code = example.get('code_ground_truth', example.get('code', ''))\n",
                "    tests_raw = example.get('unit_tests', '')\n",
                "    \n",
                "    # Parse JSON array and extract test code\n",
                "    if isinstance(tests_raw, str) and tests_raw.startswith('['):\n",
                "        try:\n",
                "            tests_list = json.loads(tests_raw)\n",
                "            test_codes = [t.get('code', '').replace('\\\\n', '\\n') for t in tests_list if t.get('code')]\n",
                "            tests = test_codes[0] if test_codes else ''\n",
                "        except:\n",
                "            tests = ''\n",
                "    else:\n",
                "        tests = tests_raw\n",
                "    \n",
                "    if not code or not tests:\n",
                "        return {\"conversations\": [], \"source_code\": \"\", \"reference_test\": \"\"}\n",
                "    \n",
                "    return {\n",
                "        \"conversations\": [\n",
                "            {\"role\": \"user\", \"content\": f\"Write pytest unit tests for:\\n```python\\n{code}\\n```\"},\n",
                "            {\"role\": \"assistant\", \"content\": f\"```python\\n{tests}\\n```\"}\n",
                "        ],\n",
                "        \"source_code\": code,\n",
                "        \"reference_test\": tests\n",
                "    }\n",
                "\n",
                "def format_testgen_qa(example):\n",
                "    \"\"\"Format HumanEval-MBPP-TestGen-QA dataset\"\"\"\n",
                "    input_text = example.get('input', '')\n",
                "    output_text = example.get('output', '')\n",
                "    \n",
                "    if not input_text or not output_text:\n",
                "        return {\"conversations\": [], \"source_code\": \"\", \"reference_test\": \"\"}\n",
                "    \n",
                "    return {\n",
                "        \"conversations\": [\n",
                "            {\"role\": \"user\", \"content\": f\"Write pytest unit tests for:\\n{input_text}\"},\n",
                "            {\"role\": \"assistant\", \"content\": output_text}\n",
                "        ],\n",
                "        \"source_code\": input_text,\n",
                "        \"reference_test\": output_text\n",
                "    }\n",
                "\n",
                "def format_humaneval(example):\n",
                "    \"\"\"Format HumanEval dataset\"\"\"\n",
                "    prompt = example.get('prompt', '')\n",
                "    canonical = example.get('canonical_solution', '')\n",
                "    test = example.get('test', '')\n",
                "    \n",
                "    full_code = prompt + canonical\n",
                "    \n",
                "    if not full_code or not test:\n",
                "        return {\"conversations\": [], \"source_code\": \"\", \"reference_test\": \"\"}\n",
                "    \n",
                "    return {\n",
                "        \"conversations\": [\n",
                "            {\"role\": \"user\", \"content\": f\"Write pytest unit tests for:\\n```python\\n{full_code}\\n```\"},\n",
                "            {\"role\": \"assistant\", \"content\": f\"```python\\n{test}\\n```\"}\n",
                "        ],\n",
                "        \"source_code\": full_code,\n",
                "        \"reference_test\": test\n",
                "    }\n",
                "\n",
                "def format_mbpp(example):\n",
                "    \"\"\"Format MBPP dataset\"\"\"\n",
                "    code = example.get('code', '')\n",
                "    test_list = example.get('test_list', [])\n",
                "    \n",
                "    # Combine test assertions\n",
                "    if isinstance(test_list, list):\n",
                "        tests = '\\n'.join(test_list)\n",
                "    else:\n",
                "        tests = str(test_list)\n",
                "    \n",
                "    if not code or not tests:\n",
                "        return {\"conversations\": [], \"source_code\": \"\", \"reference_test\": \"\"}\n",
                "    \n",
                "    # Wrap assertions in proper test function\n",
                "    test_code = f\"import pytest\\n\\ndef test_solution():\\n    {tests.replace(chr(10), chr(10) + '    ')}\"\n",
                "    \n",
                "    return {\n",
                "        \"conversations\": [\n",
                "            {\"role\": \"user\", \"content\": f\"Write pytest unit tests for:\\n```python\\n{code}\\n```\"},\n",
                "            {\"role\": \"assistant\", \"content\": f\"```python\\n{test_code}\\n```\"}\n",
                "        ],\n",
                "        \"source_code\": code,\n",
                "        \"reference_test\": test_code\n",
                "    }\n",
                "\n",
                "print(\"üîÑ Formatting datasets...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply formatting and combine\n",
                "datasets_to_merge = []\n",
                "\n",
                "if coderm:\n",
                "    coderm_fmt = coderm.map(format_coderm, remove_columns=coderm.column_names)\n",
                "    coderm_fmt = coderm_fmt.filter(lambda x: len(x['conversations']) > 0)\n",
                "    print(f\"‚úÖ CodeRM: {len(coderm_fmt)} valid samples\")\n",
                "    datasets_to_merge.append(coderm_fmt)\n",
                "\n",
                "if testgen_qa:\n",
                "    testgen_fmt = testgen_qa.map(format_testgen_qa, remove_columns=testgen_qa.column_names)\n",
                "    testgen_fmt = testgen_fmt.filter(lambda x: len(x['conversations']) > 0)\n",
                "    print(f\"‚úÖ TestGen-QA: {len(testgen_fmt)} valid samples\")\n",
                "    datasets_to_merge.append(testgen_fmt)\n",
                "\n",
                "if humaneval:\n",
                "    humaneval_fmt = humaneval.map(format_humaneval, remove_columns=humaneval.column_names)\n",
                "    humaneval_fmt = humaneval_fmt.filter(lambda x: len(x['conversations']) > 0)\n",
                "    print(f\"‚úÖ HumanEval: {len(humaneval_fmt)} valid samples\")\n",
                "    datasets_to_merge.append(humaneval_fmt)\n",
                "\n",
                "if mbpp:\n",
                "    mbpp_fmt = mbpp.map(format_mbpp, remove_columns=mbpp.column_names)\n",
                "    mbpp_fmt = mbpp_fmt.filter(lambda x: len(x['conversations']) > 0)\n",
                "    print(f\"‚úÖ MBPP: {len(mbpp_fmt)} valid samples\")\n",
                "    datasets_to_merge.append(mbpp_fmt)\n",
                "\n",
                "# Combine all datasets\n",
                "if len(datasets_to_merge) > 1:\n",
                "    combined_dataset = concatenate_datasets(datasets_to_merge).shuffle(seed=42)\n",
                "else:\n",
                "    combined_dataset = datasets_to_merge[0].shuffle(seed=42)\n",
                "\n",
                "print(f\"\\nüì¶ Combined dataset: {len(combined_dataset)} total samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create train/validation/test split\n",
                "# 90% train, 5% validation, 5% test (held out for final evaluation)\n",
                "\n",
                "total = len(combined_dataset)\n",
                "train_end = int(0.90 * total)\n",
                "val_end = int(0.95 * total)\n",
                "\n",
                "train_dataset = combined_dataset.select(range(train_end))\n",
                "val_dataset = combined_dataset.select(range(train_end, val_end))\n",
                "test_dataset = combined_dataset.select(range(val_end, total))\n",
                "\n",
                "print(f\"üìä Dataset Split:\")\n",
                "print(f\"   Train: {len(train_dataset)} samples (90%)\")\n",
                "print(f\"   Validation: {len(val_dataset)} samples (5%)\")\n",
                "print(f\"   Test (held-out): {len(test_dataset)} samples (5%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìê 6. Comprehensive Evaluation Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import tempfile\n",
                "import os\n",
                "import re\n",
                "import signal\n",
                "from contextlib import contextmanager\n",
                "\n",
                "class TimeoutException(Exception):\n",
                "    pass\n",
                "\n",
                "@contextmanager\n",
                "def timeout(seconds):\n",
                "    \"\"\"Context manager for timeout (Unix only)\"\"\"\n",
                "    def signal_handler(signum, frame):\n",
                "        raise TimeoutException(\"Timed out\")\n",
                "    signal.signal(signal.SIGALRM, signal_handler)\n",
                "    signal.alarm(seconds)\n",
                "    try:\n",
                "        yield\n",
                "    finally:\n",
                "        signal.alarm(0)\n",
                "\n",
                "def extract_code_from_response(response):\n",
                "    \"\"\"Extract Python code from markdown code blocks\"\"\"\n",
                "    if '```python' in response:\n",
                "        parts = response.split('```python')\n",
                "        if len(parts) > 1:\n",
                "            code = parts[1].split('```')[0]\n",
                "            return code.strip()\n",
                "    elif '```' in response:\n",
                "        parts = response.split('```')\n",
                "        if len(parts) > 1:\n",
                "            return parts[1].strip()\n",
                "    return response.strip()\n",
                "\n",
                "def evaluate_test_comprehensive(generated_test, source_code=None, reference_test=None):\n",
                "    \"\"\"\n",
                "    Comprehensive evaluation of generated test code.\n",
                "    Returns dict with multiple metrics.\n",
                "    \"\"\"\n",
                "    metrics = {\n",
                "        'syntax_valid': False,\n",
                "        'has_test_functions': False,\n",
                "        'has_assertions': False,\n",
                "        'has_imports': False,\n",
                "        'has_docstrings': False,\n",
                "        'executable': False,\n",
                "        'passes_on_code': False,\n",
                "        'codebleu': 0.0,\n",
                "    }\n",
                "    \n",
                "    if not generated_test or len(generated_test) < 10:\n",
                "        return metrics\n",
                "    \n",
                "    # 1. Syntax Validity\n",
                "    try:\n",
                "        compile(generated_test, '<string>', 'exec')\n",
                "        metrics['syntax_valid'] = True\n",
                "    except SyntaxError:\n",
                "        pass\n",
                "    \n",
                "    # 2. Structural Analysis\n",
                "    if re.search(r'def test_\\w+', generated_test) or re.search(r'class Test\\w+', generated_test):\n",
                "        metrics['has_test_functions'] = True\n",
                "    \n",
                "    assertion_patterns = ['assert ', 'assertEqual', 'assertTrue', 'assertFalse', \n",
                "                          'assertRaises', 'pytest.raises', 'assertIn', 'assertIsNone']\n",
                "    if any(p in generated_test for p in assertion_patterns):\n",
                "        metrics['has_assertions'] = True\n",
                "    \n",
                "    import_patterns = ['import pytest', 'import unittest', 'from unittest']\n",
                "    if any(p in generated_test for p in import_patterns):\n",
                "        metrics['has_imports'] = True\n",
                "    \n",
                "    if '\"\"\"' in generated_test or \"'''\" in generated_test:\n",
                "        metrics['has_docstrings'] = True\n",
                "    \n",
                "    # 3. Executability (can we run it without crash?)\n",
                "    if metrics['syntax_valid']:\n",
                "        try:\n",
                "            # Create isolated namespace\n",
                "            namespace = {}\n",
                "            exec(\"import pytest\\nimport unittest\", namespace)\n",
                "            # Just check if code executes (defines functions), don't run tests\n",
                "            exec(generated_test, namespace)\n",
                "            metrics['executable'] = True\n",
                "        except Exception:\n",
                "            pass\n",
                "    \n",
                "    # 4. Pass on source code (if provided)\n",
                "    if metrics['executable'] and source_code:\n",
                "        try:\n",
                "            namespace = {}\n",
                "            # Execute source code first\n",
                "            exec(source_code, namespace)\n",
                "            # Then execute test code\n",
                "            exec(generated_test, namespace)\n",
                "            # If no exception, tests likely pass\n",
                "            metrics['passes_on_code'] = True\n",
                "        except Exception:\n",
                "            pass\n",
                "    \n",
                "    # 5. CodeBLEU (if reference provided)\n",
                "    if reference_test:\n",
                "        try:\n",
                "            from codebleu import calc_codebleu\n",
                "            result = calc_codebleu([reference_test], [generated_test], lang=\"python\")\n",
                "            metrics['codebleu'] = result['codebleu']\n",
                "        except Exception:\n",
                "            # Simple token overlap fallback\n",
                "            gen_tokens = set(generated_test.split())\n",
                "            ref_tokens = set(reference_test.split())\n",
                "            if len(ref_tokens) > 0:\n",
                "                metrics['codebleu'] = len(gen_tokens & ref_tokens) / len(ref_tokens)\n",
                "    \n",
                "    return metrics\n",
                "\n",
                "def run_comprehensive_evaluation(model, tokenizer, eval_dataset, max_samples=50, desc=\"Eval\"):\n",
                "    \"\"\"\n",
                "    Run comprehensive evaluation on dataset.\n",
                "    Returns aggregated metrics.\n",
                "    \"\"\"\n",
                "    all_metrics = []\n",
                "    \n",
                "    num_samples = min(max_samples, len(eval_dataset))\n",
                "    print(f\"\\nüîç {desc}: Evaluating {num_samples} samples...\")\n",
                "    \n",
                "    for i in range(num_samples):\n",
                "        sample = eval_dataset[i]\n",
                "        source_code = sample.get('source_code', '')\n",
                "        reference_test = sample.get('reference_test', '')\n",
                "        \n",
                "        if not source_code:\n",
                "            continue\n",
                "        \n",
                "        # Generate test\n",
                "        prompt = f\"Write pytest unit tests for:\\n```python\\n{source_code[:1500]}\\n```\"\n",
                "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
                "        \n",
                "        inputs = tokenizer.apply_chat_template(\n",
                "            messages, \n",
                "            return_tensors=\"pt\",\n",
                "            add_generation_prompt=True\n",
                "        ).to(\"cuda\")\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = model.generate(\n",
                "                inputs, \n",
                "                max_new_tokens=512, \n",
                "                temperature=0.7,\n",
                "                do_sample=True,\n",
                "                pad_token_id=tokenizer.pad_token_id,\n",
                "            )\n",
                "        \n",
                "        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
                "        generated_test = extract_code_from_response(response)\n",
                "        \n",
                "        # Evaluate\n",
                "        metrics = evaluate_test_comprehensive(generated_test, source_code, reference_test)\n",
                "        all_metrics.append(metrics)\n",
                "        \n",
                "        if (i + 1) % 10 == 0:\n",
                "            print(f\"   Progress: {i + 1}/{num_samples}\")\n",
                "    \n",
                "    # Aggregate results\n",
                "    if not all_metrics:\n",
                "        return None\n",
                "    \n",
                "    aggregated = {}\n",
                "    for key in all_metrics[0].keys():\n",
                "        values = [m[key] for m in all_metrics]\n",
                "        if isinstance(values[0], bool):\n",
                "            aggregated[key] = sum(values) / len(values) * 100  # Percentage\n",
                "        else:\n",
                "            aggregated[key] = sum(values) / len(values) * 100  # Also percentage\n",
                "    \n",
                "    # Overall score (weighted average)\n",
                "    aggregated['overall'] = (\n",
                "        aggregated['syntax_valid'] * 0.15 +\n",
                "        aggregated['has_test_functions'] * 0.15 +\n",
                "        aggregated['has_assertions'] * 0.15 +\n",
                "        aggregated['has_imports'] * 0.05 +\n",
                "        aggregated['executable'] * 0.20 +\n",
                "        aggregated['passes_on_code'] * 0.20 +\n",
                "        aggregated['codebleu'] * 0.10\n",
                "    )\n",
                "    \n",
                "    return aggregated\n",
                "\n",
                "def print_metrics_table(metrics, title):\n",
                "    \"\"\"Pretty print metrics\"\"\"\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"üìä {title}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    print(f\"{'Metric':<25} {'Score':>10}\")\n",
                "    print(f\"{'-'*35}\")\n",
                "    for key, value in metrics.items():\n",
                "        print(f\"{key:<25} {value:>9.1f}%\")\n",
                "    print(f\"{'='*60}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìè 7. BASELINE Evaluation (Before Fine-tuning)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"üî¥\"*30)\n",
                "print(\"BASELINE EVALUATION - Before Fine-tuning\")\n",
                "print(\"üî¥\"*30)\n",
                "\n",
                "# Evaluate on held-out test set\n",
                "baseline_scores = run_comprehensive_evaluation(\n",
                "    model, \n",
                "    tokenizer, \n",
                "    test_dataset, \n",
                "    max_samples=50,\n",
                "    desc=\"BASELINE\"\n",
                ")\n",
                "\n",
                "if baseline_scores:\n",
                "    print_metrics_table(baseline_scores, \"BASELINE Scores (Gemma 3 12B - Before Fine-tuning)\")\n",
                "else:\n",
                "    print(\"‚ùå Baseline evaluation failed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìù 8. Apply Chat Template"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth.chat_templates import get_chat_template, standardize_data_formats\n",
                "\n",
                "tokenizer = get_chat_template(\n",
                "    tokenizer,\n",
                "    chat_template=\"gemma-3\",\n",
                ")\n",
                "\n",
                "# Standardize format\n",
                "train_dataset = standardize_data_formats(train_dataset)\n",
                "\n",
                "def formatting_prompts_func(examples):\n",
                "    convos = examples[\"conversations\"]\n",
                "    texts = [\n",
                "        tokenizer.apply_chat_template(\n",
                "            convo, \n",
                "            tokenize=False, \n",
                "            add_generation_prompt=False\n",
                "        ).removeprefix('<bos>')\n",
                "        for convo in convos\n",
                "    ]\n",
                "    return {\"text\": texts}\n",
                "\n",
                "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
                "print(f\"‚úÖ Applied Gemma-3 chat template to {len(train_dataset)} training samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ 9. Setup LoRA Adapters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = FastModel.get_peft_model(\n",
                "    model,\n",
                "    r=16,\n",
                "    target_modules=[\n",
                "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
                "    ],\n",
                "    lora_alpha=16,\n",
                "    lora_dropout=0,\n",
                "    bias=\"none\",\n",
                "    use_gradient_checkpointing=\"unsloth\",\n",
                "    random_state=42,\n",
                "    use_rslora=False,\n",
                "    loftq_config=None,\n",
                ")\n",
                "\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "print(f\"\\n‚úÖ LoRA adapters configured\")\n",
                "print(f\"   Trainable: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
                "print(f\"   Total: {total_params:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚ö° 10. Training Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from trl import SFTTrainer, SFTConfig\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=None,\n",
                "    args=SFTConfig(\n",
                "        dataset_text_field=\"text\",\n",
                "        per_device_train_batch_size=2,\n",
                "        gradient_accumulation_steps=4,  # Effective batch = 8\n",
                "        warmup_steps=100,\n",
                "        num_train_epochs=1,\n",
                "        learning_rate=2e-4,\n",
                "        logging_steps=50,\n",
                "        optim=\"adamw_8bit\",\n",
                "        weight_decay=0.01,\n",
                "        lr_scheduler_type=\"cosine\",\n",
                "        seed=42,\n",
                "        output_dir=\"./gemma3-12b-testgen\",\n",
                "        save_steps=500,\n",
                "        report_to=\"none\",\n",
                "    ),\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Trainer configured\")\n",
                "print(f\"   Dataset: {len(train_dataset)} samples\")\n",
                "print(f\"   Effective batch size: 8 (2 √ó 4)\")\n",
                "print(f\"   Epochs: 1\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth.chat_templates import train_on_responses_only\n",
                "\n",
                "trainer = train_on_responses_only(\n",
                "    trainer,\n",
                "    instruction_part=\"<start_of_turn>user\\n\",\n",
                "    response_part=\"<start_of_turn>model\\n\",\n",
                ")\n",
                "print(\"‚úÖ Configured to train on responses only\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üöÄ 11. Start Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gpu_stats = torch.cuda.get_device_properties(0)\n",
                "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
                "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
                "print(f\"GPU = {gpu_stats.name}\")\n",
                "print(f\"Max memory = {max_memory} GB\")\n",
                "print(f\"Reserved = {start_gpu_memory} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"üöÄ\"*30)\n",
                "print(\"STARTING TRAINING\")\n",
                "print(\"üöÄ\"*30 + \"\\n\")\n",
                "\n",
                "trainer_stats = trainer.train()\n",
                "\n",
                "print(\"\\n\" + \"‚úÖ\"*30)\n",
                "print(\"TRAINING COMPLETE!\")\n",
                "print(\"‚úÖ\"*30)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
                "print(\"\\nüìä Training Summary\")\n",
                "print(\"=\"*50)\n",
                "print(f\"   Final loss: {trainer_stats.training_loss:.4f}\")\n",
                "print(f\"   Training time: {trainer_stats.metrics['train_runtime']/60:.1f} minutes\")\n",
                "print(f\"   Peak memory: {used_memory} GB\")\n",
                "print(f\"   Samples/sec: {trainer_stats.metrics['train_samples_per_second']:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìè 12. POST-TRAINING Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"üü¢\"*30)\n",
                "print(\"POST-TRAINING EVALUATION - After Fine-tuning\")\n",
                "print(\"üü¢\"*30)\n",
                "\n",
                "model.eval()\n",
                "\n",
                "finetuned_scores = run_comprehensive_evaluation(\n",
                "    model, \n",
                "    tokenizer, \n",
                "    test_dataset, \n",
                "    max_samples=50,\n",
                "    desc=\"FINE-TUNED\"\n",
                ")\n",
                "\n",
                "if finetuned_scores:\n",
                "    print_metrics_table(finetuned_scores, \"FINE-TUNED Scores (Gemma 3 12B - After Training)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Side-by-side comparison\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"üìà COMPARISON: Baseline vs Fine-tuned\")\n",
                "print(\"=\"*70)\n",
                "print(f\"{'Metric':<25} {'Baseline':>12} {'Fine-tuned':>12} {'Œî Change':>12}\")\n",
                "print(\"-\"*70)\n",
                "\n",
                "if baseline_scores and finetuned_scores:\n",
                "    for metric in baseline_scores.keys():\n",
                "        base = baseline_scores[metric]\n",
                "        fine = finetuned_scores[metric]\n",
                "        delta = fine - base\n",
                "        arrow = \"‚Üë\" if delta > 0 else \"‚Üì\" if delta < 0 else \"‚Üí\"\n",
                "        color = \"\" if delta >= 0 else \"\"\n",
                "        print(f\"{metric:<25} {base:>11.1f}% {fine:>11.1f}% {arrow:>3} {delta:>+7.1f}%\")\n",
                "\n",
                "    print(\"=\"*70)\n",
                "    improvement = finetuned_scores['overall'] - baseline_scores['overall']\n",
                "    print(f\"\\nüéØ OVERALL IMPROVEMENT: {improvement:+.1f}%\")\n",
                "    print(f\"   Baseline: {baseline_scores['overall']:.1f}%\")\n",
                "    print(f\"   Fine-tuned: {finetuned_scores['overall']:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üíæ 13. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.save_pretrained(\"gemma3-12b-testgen-lora\")\n",
                "tokenizer.save_pretrained(\"gemma3-12b-testgen-lora\")\n",
                "print(\"‚úÖ LoRA adapters saved to: gemma3-12b-testgen-lora/\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üì¶ Exporting to GGUF (Q4_K_M for RTX 4080)...\")\n",
                "print(\"   This may take 10-15 minutes...\")\n",
                "\n",
                "model.save_pretrained_gguf(\n",
                "    \"gemma3-12b-testgen-gguf\",\n",
                "    tokenizer,\n",
                "    quantization_method=\"q4_k_m\"\n",
                ")\n",
                "print(\"\\n‚úÖ GGUF model saved to: gemma3-12b-testgen-gguf/\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß™ 14. Test Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_code = '''\n",
                "class LRUCache:\n",
                "    def __init__(self, capacity: int):\n",
                "        self.capacity = capacity\n",
                "        self.cache = {}\n",
                "        self.order = []\n",
                "    \n",
                "    def get(self, key: int) -> int:\n",
                "        if key not in self.cache:\n",
                "            return -1\n",
                "        self.order.remove(key)\n",
                "        self.order.append(key)\n",
                "        return self.cache[key]\n",
                "    \n",
                "    def put(self, key: int, value: int) -> None:\n",
                "        if key in self.cache:\n",
                "            self.order.remove(key)\n",
                "        elif len(self.cache) >= self.capacity:\n",
                "            oldest = self.order.pop(0)\n",
                "            del self.cache[oldest]\n",
                "        self.cache[key] = value\n",
                "        self.order.append(key)\n",
                "'''\n",
                "\n",
                "messages = [{\"role\": \"user\", \"content\": f\"Write pytest unit tests for:\\n```python\\n{test_code}\\n```\"}]\n",
                "\n",
                "inputs = tokenizer.apply_chat_template(\n",
                "    messages, \n",
                "    return_tensors=\"pt\",\n",
                "    add_generation_prompt=True\n",
                ").to(\"cuda\")\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs = model.generate(\n",
                "        inputs, \n",
                "        max_new_tokens=1024, \n",
                "        temperature=0.7,\n",
                "        do_sample=True,\n",
                "    )\n",
                "\n",
                "response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
                "\n",
                "print(\"üß™ Generated Tests for LRUCache:\")\n",
                "print(\"=\"*60)\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìã 15. Final Report & Resume Bullet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"üéØ FINAL TRAINING REPORT\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(f\"\"\"\n",
                "MODEL DETAILS:\n",
                "  Base Model: Gemma 3 12B\n",
                "  Method: QLoRA (4-bit quantization)\n",
                "  LoRA Rank: 16\n",
                "  \n",
                "DATASET:\n",
                "  Total Samples: {len(combined_dataset):,}\n",
                "  Training: {len(train_dataset):,} (90%)\n",
                "  Validation: {len(val_dataset):,} (5%)\n",
                "  Test: {len(test_dataset):,} (5%)\n",
                "  Sources: CodeRM-UnitTest + HumanEval + MBPP\n",
                "\n",
                "TRAINING:\n",
                "  Final Loss: {trainer_stats.training_loss:.4f}\n",
                "  Training Time: {trainer_stats.metrics['train_runtime']/60:.1f} minutes\n",
                "  Samples/sec: {trainer_stats.metrics['train_samples_per_second']:.2f}\n",
                "\"\"\")\n",
                "\n",
                "if baseline_scores and finetuned_scores:\n",
                "    improvement = finetuned_scores['overall'] - baseline_scores['overall']\n",
                "    print(f\"\"\"EVALUATION RESULTS:\n",
                "  Baseline Overall: {baseline_scores['overall']:.1f}%\n",
                "  Fine-tuned Overall: {finetuned_scores['overall']:.1f}%\n",
                "  Improvement: {improvement:+.1f}%\n",
                "  \n",
                "  Key Metrics Improvement:\n",
                "    Syntax Valid: {baseline_scores['syntax_valid']:.1f}% ‚Üí {finetuned_scores['syntax_valid']:.1f}%\n",
                "    Has Test Functions: {baseline_scores['has_test_functions']:.1f}% ‚Üí {finetuned_scores['has_test_functions']:.1f}%\n",
                "    Executable: {baseline_scores['executable']:.1f}% ‚Üí {finetuned_scores['executable']:.1f}%\n",
                "    Passes on Code: {baseline_scores['passes_on_code']:.1f}% ‚Üí {finetuned_scores['passes_on_code']:.1f}%\n",
                "\"\"\")\n",
                "\n",
                "print(f\"\"\"\n",
                "EXPORTS:\n",
                "  LoRA Adapters: gemma3-12b-testgen-lora/\n",
                "  GGUF (Q4_K_M): gemma3-12b-testgen-gguf/ (~6GB)\n",
                "\n",
                "LOCAL DEPLOYMENT:\n",
                "  ollama create testgen -f Modelfile\n",
                "  ollama run testgen\n",
                "\"\"\")\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"üìù RESUME BULLET POINT:\")\n",
                "print(\"=\"*70)\n",
                "if baseline_scores and finetuned_scores:\n",
                "    print(f\"\"\"\n",
                "Fine-tuned Gemma 3 12B using QLoRA on {len(train_dataset):,} curated code-test pairs \n",
                "from CodeRM, HumanEval, and MBPP datasets. Achieved {finetuned_scores['overall']:.0f}% \n",
                "test quality score ({improvement:+.0f}% over baseline), with {finetuned_scores['executable']:.0f}% \n",
                "executable tests and {finetuned_scores['syntax_valid']:.0f}% syntax validity. Deployed \n",
                "locally via GGUF quantization on RTX 4080.\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì• 16. Download Model Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!zip -r gemma3-12b-testgen-lora.zip gemma3-12b-testgen-lora/\n",
                "!zip -r gemma3-12b-testgen-gguf.zip gemma3-12b-testgen-gguf/\n",
                "\n",
                "print(\"\\nüì• Files ready for download:\")\n",
                "print(\"  - gemma3-12b-testgen-lora.zip\")\n",
                "print(\"  - gemma3-12b-testgen-gguf.zip\")\n",
                "\n",
                "try:\n",
                "    from google.colab import files\n",
                "    files.download('gemma3-12b-testgen-gguf.zip')\n",
                "except:\n",
                "    print(\"\\n‚ö†Ô∏è Not in Colab - use the file browser to download\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}