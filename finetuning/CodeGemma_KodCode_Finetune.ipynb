{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# CodeGemma 7B Fine-tuning with KodCode (Unsloth)\n",
                "\n",
                "**Goal:** Fine-tune CodeGemma-7B for pytest test generation using KodCode dataset.\n",
                "\n",
                "**Requirements:**\n",
                "- GPU: A100 (40GB) recommended, L4 (24GB) works, T4 (15GB) works with smaller batch\n",
                "- Runtime: ~2-4 hours for 1 epoch on 30k samples\n",
                "\n",
                "**What this notebook does:**\n",
                "1. Loads `unsloth/codegemma-7b-bnb-4bit` (4-bit quantized)\n",
                "2. Adds LoRA adapters for efficient fine-tuning\n",
                "3. Loads KodCode-V1-SFT-R1 dataset (verified test cases)\n",
                "4. Trains with SFTTrainer (1 epoch)\n",
                "5. Saves LoRA adapter + exports to GGUF for Ollama"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install"
            },
            "outputs": [],
            "source": [
                "%%capture\n",
                "import os, re\n",
                "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
                "    !pip install unsloth\n",
                "else:\n",
                "    # Colab-specific installation\n",
                "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
                "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
                "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
                "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
                "    !pip install --no-deps unsloth\n",
                "!pip install transformers==4.56.2\n",
                "!pip install --no-deps trl==0.22.2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration\n",
                "\n",
                "**Adjust these based on your GPU:**\n",
                "- A100 (40GB): `batch_size=4`, `max_seq_length=4096`\n",
                "- L4 (24GB): `batch_size=2`, `max_seq_length=2048`\n",
                "- T4 (15GB): `batch_size=1`, `max_seq_length=2048`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "config"
            },
            "outputs": [],
            "source": [
                "# ============ CONFIGURATION ============\n",
                "# Optimized for H100 (80GB VRAM)\n",
                "\n",
                "# Model\n",
                "MODEL_NAME = \"unsloth/codegemma-7b-bnb-4bit\"\n",
                "MAX_SEQ_LENGTH = 4096    # ← Changed from 2048\n",
                "LOAD_IN_4BIT = True\n",
                "\n",
                "# LoRA\n",
                "LORA_R = 16\n",
                "LORA_ALPHA = 16\n",
                "LORA_DROPOUT = 0\n",
                "\n",
                "# Dataset\n",
                "DATA_SOURCE = \"kodcode\"\n",
                "MAX_SAMPLES = 30_000\n",
                "FILTER_BY_LENGTH = True\n",
                "\n",
                "# Training - H100 optimal\n",
                "BATCH_SIZE = 8           # ← Changed from 2\n",
                "GRAD_ACCUM_STEPS = 2     # ← Changed from 4\n",
                "NUM_EPOCHS = 1\n",
                "LEARNING_RATE = 2e-4\n",
                "WARMUP_RATIO = 0.03\n",
                "USE_PACKING = True\n",
                "\n",
                "# Output\n",
                "OUTPUT_DIR = \"./codegemma-kodcode-lora\"\n",
                "HF_REPO = None\n",
                "SAVE_GGUF = True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Model + LoRA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "load_model"
            },
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "\n",
                "# Auto-detect dtype\n",
                "dtype = None  # Float16 for T4/V100, Bfloat16 for Ampere+\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=MODEL_NAME,\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    dtype=dtype,\n",
                "    load_in_4bit=LOAD_IN_4BIT,\n",
                "    # token=\"hf_...\",  # Uncomment if using gated models\n",
                ")\n",
                "\n",
                "print(f\"Loaded {MODEL_NAME}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "add_lora"
            },
            "outputs": [],
            "source": [
                "# Add LoRA adapters (only 1-10% of parameters are trainable)\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r=LORA_R,\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha=LORA_ALPHA,\n",
                "    lora_dropout=LORA_DROPOUT,\n",
                "    bias=\"none\",\n",
                "    use_gradient_checkpointing=\"unsloth\",  # 30% less VRAM\n",
                "    random_state=3407,\n",
                "    use_rslora=False,\n",
                "    loftq_config=None,\n",
                ")\n",
                "\n",
                "model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Prepare Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "prepare_data"
            },
            "outputs": [],
            "source": [
                "from unsloth.chat_templates import get_chat_template\n",
                "from datasets import load_dataset, Dataset\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Apply ChatML template (CodeGemma uses this format)\n",
                "tokenizer = get_chat_template(\n",
                "    tokenizer,\n",
                "    chat_template=\"chatml\",\n",
                "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
                "    map_eos_token=True,\n",
                ")\n",
                "\n",
                "# Prompt template for test generation\n",
                "PROMPT_TEMPLATE = \"\"\"### Task: Write pytest tests\n",
                "\n",
                "### Code Under Test:\n",
                "{code}\n",
                "\n",
                "### Constraints:\n",
                "pytest only, no hypothesis, no unittest. Edge cases, parametrize, no explanation.\"\"\"\n",
                "\n",
                "def formatting_prompts_func(examples):\n",
                "    \"\"\"Convert conversations to ChatML text format.\"\"\"\n",
                "    convos = examples[\"conversations\"]\n",
                "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
                "    return {\"text\": texts}\n",
                "\n",
                "# Load dataset\n",
                "print(f\"Loading {DATA_SOURCE} dataset...\")\n",
                "\n",
                "if DATA_SOURCE == \"guanaco\":\n",
                "    # Demo dataset (general conversation)\n",
                "    dataset = load_dataset(\"philschmid/guanaco-sharegpt-style\", split=\"train\")\n",
                "    if MAX_SAMPLES < len(dataset):\n",
                "        dataset = dataset.shuffle(seed=42).select(range(MAX_SAMPLES))\n",
                "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
                "else:\n",
                "    # KodCode or TestGenEval\n",
                "    if DATA_SOURCE == \"kodcode\":\n",
                "        ds = load_dataset(\"KodCode/KodCode-V1-SFT-R1\", split=\"train\")\n",
                "        print(f\"KodCode columns: {ds.column_names}\")\n",
                "        \n",
                "        # Flexible column mapping\n",
                "        code_col = next((c for c in (\"solution\", \"r1_solution\", \"code\") if c in ds.column_names), None)\n",
                "        test_col = next((c for c in (\"test\", \"tests\") if c in ds.column_names), None)\n",
                "        \n",
                "        if not code_col or not test_col:\n",
                "            raise KeyError(f\"Required columns not found. Need code+test. Got: {ds.column_names}\")\n",
                "        print(f\"Using columns: code={code_col}, test={test_col}\")\n",
                "        \n",
                "        # Filter by correctness if available\n",
                "        if \"r1_correctness\" in ds.column_names:\n",
                "            before = len(ds)\n",
                "            ds = ds.filter(lambda x: x.get(\"r1_correctness\") in (True, \"True\", \"true\", 1))\n",
                "            print(f\"Filtered by r1_correctness: {before} -> {len(ds)}\")\n",
                "    else:\n",
                "        # TestGenEval\n",
                "        ds = load_dataset(\"kjain14/testgeneval\", split=\"train\")\n",
                "        code_col, test_col = \"code_src\", \"test_src\"\n",
                "    \n",
                "    # Convert to conversation format with length filtering\n",
                "    rows = []\n",
                "    skipped_long = 0\n",
                "    skipped_empty = 0\n",
                "    \n",
                "    for row in tqdm(ds, desc=\"Processing\"):\n",
                "        code = str(row.get(code_col, \"\") or \"\").strip()\n",
                "        test = str(row.get(test_col, \"\") or \"\").strip()\n",
                "        \n",
                "        if not code or not test:\n",
                "            skipped_empty += 1\n",
                "            continue\n",
                "        \n",
                "        prompt = PROMPT_TEMPLATE.format(code=code)\n",
                "        full_text = prompt + \"\\n\" + test\n",
                "        \n",
                "        # Skip if too long\n",
                "        if FILTER_BY_LENGTH:\n",
                "            n_tokens = len(tokenizer.encode(full_text, add_special_tokens=False))\n",
                "            if n_tokens > MAX_SEQ_LENGTH - 50:  # Leave room for special tokens\n",
                "                skipped_long += 1\n",
                "                continue\n",
                "        \n",
                "        rows.append({\n",
                "            \"conversations\": [\n",
                "                {\"from\": \"human\", \"value\": prompt},\n",
                "                {\"from\": \"gpt\", \"value\": test}\n",
                "            ]\n",
                "        })\n",
                "        \n",
                "        if len(rows) >= MAX_SAMPLES:\n",
                "            break\n",
                "    \n",
                "    print(f\"\\nDataset stats:\")\n",
                "    print(f\"  Loaded: {len(rows)} samples\")\n",
                "    print(f\"  Skipped (empty): {skipped_empty}\")\n",
                "    print(f\"  Skipped (too long): {skipped_long}\")\n",
                "    \n",
                "    dataset = Dataset.from_list(rows)\n",
                "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
                "\n",
                "print(f\"\\nFinal dataset size: {len(dataset)} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "preview_data"
            },
            "outputs": [],
            "source": [
                "# Preview a sample\n",
                "print(\"=\" * 60)\n",
                "print(\"Sample conversation:\")\n",
                "print(\"=\" * 60)\n",
                "print(dataset[0][\"text\"][:2000])\n",
                "print(\"...\" if len(dataset[0][\"text\"]) > 2000 else \"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train_setup"
            },
            "outputs": [],
            "source": [
                "from trl import SFTTrainer, SFTConfig\n",
                "\n",
                "# Calculate total steps\n",
                "total_samples = len(dataset)\n",
                "effective_batch = BATCH_SIZE * GRAD_ACCUM_STEPS\n",
                "steps_per_epoch = total_samples // effective_batch\n",
                "total_steps = int(steps_per_epoch * NUM_EPOCHS)\n",
                "\n",
                "print(f\"Training config:\")\n",
                "print(f\"  Samples: {total_samples}\")\n",
                "print(f\"  Effective batch size: {effective_batch}\")\n",
                "print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
                "print(f\"  Total steps: {total_steps}\")\n",
                "print(f\"  Warmup steps: {int(total_steps * WARMUP_RATIO)}\")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    train_dataset=dataset,\n",
                "    packing=USE_PACKING,\n",
                "    args=SFTConfig(\n",
                "        output_dir=OUTPUT_DIR,\n",
                "        per_device_train_batch_size=BATCH_SIZE,\n",
                "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
                "        num_train_epochs=NUM_EPOCHS,\n",
                "        learning_rate=LEARNING_RATE,\n",
                "        warmup_ratio=WARMUP_RATIO,\n",
                "        optim=\"adamw_8bit\",\n",
                "        weight_decay=0.01,\n",
                "        lr_scheduler_type=\"cosine\",\n",
                "        seed=3407,\n",
                "        max_seq_length=MAX_SEQ_LENGTH,\n",
                "        dataset_text_field=\"text\",\n",
                "        report_to=\"none\",  # Set to \"wandb\" if you want logging\n",
                "        logging_steps=50,\n",
                "        save_steps=500,\n",
                "        save_total_limit=2,\n",
                "        fp16=not torch.cuda.is_bf16_supported(),\n",
                "        bf16=torch.cuda.is_bf16_supported(),\n",
                "    ),\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "show_memory"
            },
            "outputs": [],
            "source": [
                "# Show memory before training\n",
                "gpu_stats = torch.cuda.get_device_properties(0)\n",
                "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 3)\n",
                "max_memory = round(gpu_stats.total_memory / 1024**3, 3)\n",
                "print(f\"GPU: {gpu_stats.name}\")\n",
                "print(f\"Max memory: {max_memory} GB\")\n",
                "print(f\"Reserved before training: {start_gpu_memory} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train"
            },
            "outputs": [],
            "source": [
                "# Train!\n",
                "print(\"Starting training...\")\n",
                "trainer_stats = trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "show_stats"
            },
            "outputs": [],
            "source": [
                "# Show final stats\n",
                "used_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 3)\n",
                "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
                "used_percentage = round(used_memory / max_memory * 100, 3)\n",
                "\n",
                "print(f\"\\nTraining complete!\")\n",
                "print(f\"  Time: {trainer_stats.metrics['train_runtime']:.1f} seconds ({trainer_stats.metrics['train_runtime']/60:.1f} min)\")\n",
                "print(f\"  Final loss: {trainer_stats.metrics.get('train_loss', 'N/A')}\")\n",
                "print(f\"  Peak memory: {used_memory} GB ({used_percentage}% of {max_memory} GB)\")\n",
                "print(f\"  Memory for training: {used_memory_for_lora} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Test the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "inference"
            },
            "outputs": [],
            "source": [
                "from unsloth.chat_templates import get_chat_template\n",
                "from transformers import TextStreamer\n",
                "\n",
                "# Re-apply chat template for inference\n",
                "tokenizer = get_chat_template(\n",
                "    tokenizer,\n",
                "    chat_template=\"chatml\",\n",
                "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
                "    map_eos_token=True,\n",
                ")\n",
                "\n",
                "# Enable faster inference\n",
                "FastLanguageModel.for_inference(model)\n",
                "\n",
                "# Test prompt\n",
                "test_code = '''\n",
                "def fibonacci(n: int) -> int:\n",
                "    \"\"\"Return the nth Fibonacci number.\"\"\"\n",
                "    if n <= 1:\n",
                "        return n\n",
                "    return fibonacci(n - 1) + fibonacci(n - 2)\n",
                "'''\n",
                "\n",
                "test_prompt = PROMPT_TEMPLATE.format(code=test_code)\n",
                "\n",
                "messages = [{\"from\": \"human\", \"value\": test_prompt}]\n",
                "inputs = tokenizer.apply_chat_template(\n",
                "    messages,\n",
                "    tokenize=True,\n",
                "    add_generation_prompt=True,\n",
                "    return_tensors=\"pt\",\n",
                ").to(\"cuda\")\n",
                "\n",
                "print(\"Generating tests for fibonacci function...\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
                "_ = model.generate(\n",
                "    input_ids=inputs,\n",
                "    streamer=text_streamer,\n",
                "    max_new_tokens=512,\n",
                "    use_cache=True,\n",
                "    temperature=0.1,\n",
                "    top_p=0.9,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "save_lora"
            },
            "outputs": [],
            "source": [
                "# Save LoRA adapter locally\n",
                "print(f\"Saving LoRA adapter to {OUTPUT_DIR}...\")\n",
                "model.save_pretrained(OUTPUT_DIR)\n",
                "tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "print(\"Done!\")\n",
                "\n",
                "# Push to HuggingFace (optional)\n",
                "if HF_REPO:\n",
                "    print(f\"Pushing to HuggingFace: {HF_REPO}...\")\n",
                "    model.push_to_hub(HF_REPO, token=os.environ.get(\"HF_TOKEN\"))\n",
                "    tokenizer.push_to_hub(HF_REPO, token=os.environ.get(\"HF_TOKEN\"))\n",
                "    print(\"Done!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "save_merged"
            },
            "outputs": [],
            "source": [
                "# Save merged 16-bit model (for vLLM or direct inference)\n",
                "if False:  # Set to True to save merged model\n",
                "    print(\"Saving merged 16-bit model...\")\n",
                "    model.save_pretrained_merged(f\"{OUTPUT_DIR}-merged\", tokenizer, save_method=\"merged_16bit\")\n",
                "    print(\"Done!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "save_gguf"
            },
            "outputs": [],
            "source": [
                "# Save GGUF for Ollama\n",
                "if SAVE_GGUF:\n",
                "    print(\"Saving GGUF (q4_k_m) for Ollama...\")\n",
                "    model.save_pretrained_gguf(\n",
                "        f\"{OUTPUT_DIR}-gguf\",\n",
                "        tokenizer,\n",
                "        quantization_method=\"q4_k_m\"  # Good balance of size/quality\n",
                "    )\n",
                "    print(f\"Done! GGUF saved to {OUTPUT_DIR}-gguf/\")\n",
                "    print(\"\\nTo use in Ollama:\")\n",
                "    print(f\"  1. Copy the .gguf file to your local machine\")\n",
                "    print(f\"  2. Create a Modelfile with: FROM ./{OUTPUT_DIR}-gguf/unsloth.Q4_K_M.gguf\")\n",
                "    print(f\"  3. Run: ollama create codegemma-kodcode -f Modelfile\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Download Files\n",
                "\n",
                "Run this cell to create a zip file you can download from Colab."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download"
            },
            "outputs": [],
            "source": [
                "import shutil\n",
                "\n",
                "# Create zip of LoRA adapter\n",
                "print(\"Creating zip file...\")\n",
                "shutil.make_archive(f\"{OUTPUT_DIR}\", 'zip', OUTPUT_DIR)\n",
                "print(f\"Created {OUTPUT_DIR}.zip\")\n",
                "\n",
                "# In Colab, you can download with:\n",
                "try:\n",
                "    from google.colab import files\n",
                "    files.download(f\"{OUTPUT_DIR}.zip\")\n",
                "except:\n",
                "    print(f\"Download {OUTPUT_DIR}.zip manually from the file browser.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## GPU Recommendations\n",
                "\n",
                "| GPU | VRAM | Batch Size | Seq Length | Est. Time (30k samples) |\n",
                "|-----|------|------------|------------|-------------------------|\n",
                "| **A100 (40GB)** | 40GB | 4 | 4096 | ~1.5 hours |\n",
                "| **A100 (80GB)** | 80GB | 8 | 4096 | ~1 hour |\n",
                "| **L4** | 24GB | 2 | 2048 | ~2.5 hours |\n",
                "| **T4** | 15GB | 1 | 2048 | ~4 hours |\n",
                "\n",
                "### Cloud Options:\n",
                "\n",
                "1. **Google Colab Pro** ($10/month): A100 access, good for experimentation\n",
                "2. **Google Colab Pro+** ($50/month): Longer runtimes, priority A100\n",
                "3. **RunPod**: A100 ~$1.99/hr, good for long training\n",
                "4. **Lambda Labs**: A100 ~$1.29/hr (often waitlisted)\n",
                "5. **Vast.ai**: Cheapest A100s ~$0.80-1.50/hr (variable quality)\n",
                "\n",
                "**Recommendation:** Start with Colab Pro (A100) for prototyping, use RunPod for final training."
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
